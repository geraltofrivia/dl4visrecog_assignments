# KNN

### Q1:

- Demo KNN works with 99.91% accuracy.

### Q2 a,b - all classes:

9.70 of test examples classified correctly. k= 1
Confusion matrix:
[[118 125  68 135 135  36  70 106  88  99]
 [104 162 141  96 136  65  86 122 103 120]
 [120 124  98 115 147 107 118  58  61  84]
 [ 39  89 126  69 115  81 119 204  55 113]
 [ 93 112 101  70 109 107  92 126  71 101]
 [ 34 123 138  71  68 106  89  74  95  94]
 [103 111  99  71 107 107  65 160  76  59]
 [150  68  41  76 128  88  90  97 199  91]
 [122  51 117 170  85  95  80 113  66  75]
5  73  7 [ 97 170 112  88  87 164  63  80]]

9.68 of test examples classified correctly. k= 5
Confusion matrix:
[[219 234  59 140 104  21  48  82  31  42]
 [206 271 188  71 103  68  42  44  79  63]
 [279 229 107 103 102  73  57  17  23  42]
 [101 186 202  41  86  26  72 204  21  71]
 [206 195 108  46 106  75  83  96  42  25]
 [100 207 194  70  36  81  48  53  41  62]
 [279 137 185  46  95  46  19 108  14  29]
 [319 156  33  33  84  48  70  50 195  40]
 [289  94 153 105  83  65  39  74  27  45]
 [187 276  65  61 122  42  62 124  23  47]]

Why would it be so low? Given that we got near perfect score on two class classification. Well I believe it happens since we have only 100 examples per class.
Why then would the other thing be so damned high? Well it is radically easy to differentiate b/w 0 and 1. Probably the most distinct of all numbers. Here the contention is b/w 10.
But a probability of 10% simply means that the labels are being assigned randomly. Weird.


#### Q2 c - cross validation:
NOTE: Testing done on the trainset's splits as well. (800,200) splits. NOT tested on x_test
o/p (accuracy) of each split:
[ 0.135  0.125  0.125  0.14   0.125  0.105  0.135  0.135  0.125  0.115  0.11   0.14   0.145  0.135  0.125]
[ 0.11   0.11   0.075  0.11   0.115  0.115  0.115  0.09   0.11   0.115  0.065  0.07   0.08   0.075  0.105]
[ 0.095  0.105  0.09   0.085  0.075  0.095  0.095  0.105  0.105  0.125  0.125  0.115  0.12   0.12   0.11 ]
[ 0.105  0.11   0.07   0.08   0.085  0.08   0.1    0.095  0.085  0.08   0.09   0.095  0.08   0.095  0.09 ]
[ 0.125  0.11   0.105  0.105  0.065  0.085  0.08   0.09   0.1    0.12   0.11   0.12   0.135  0.135  0.145]

avg:
[ 0.114  0.112  0.093  0.104  0.093  0.096  0.105  0.103  0.105  0.111  0.1    0.108  0.112  0.112  0.115]

winner: k = 15

random since k = 7 was a winner sometime back. Also this is counter intuitive to the previous experiment. Very very weird.
Or maybe... I have cracked the code to generate pure random numbers.

**plot**: 5fcv_results


#### Q2 d - All classes, all examples
NOTE: It is ambigous whether I have to perform cross validation, or not. I am assuming that I do not. 
Using X_train, X_test in this case now.


# Logistic Regression

### Q1: Derivatives

_s(x)_ = 1 / (1 + e^(-x) )

u = wx + b
v = s( u )
l = (y - v)^2

dl/dv = -2 ( y - v )
dl/du = dl/dv * dv/du
      = dl/dv * s(u) * (1 - s(u))
**dl/dw = x dl/du**
**dl/db = dl/du**

### Q2: Implement

Done
`99.802605606 % of test examples classified correctly.`

### Q3: Plot
`99.9368337939 % of test examples classified correctly.`
t1000_sigmoid_results.png -> plot