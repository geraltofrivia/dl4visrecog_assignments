# KNN

### Q1:

- Demo KNN works with 99.91% accuracy.

### Q2 a,b - all classes:

9.70 of test examples classified correctly. k = 1
[[131 163  28 105  79  77 138  98  71  90]
 [ 83 101 130 107 163  93  69  83 152 154]
 [126 156  81 101  79  78 121 122  75  93]
 [ 36  94 204  91 110  83  81 116  75 120]
 [ 99 161  63 122  59  78  81  92 105 122]
 [ 84  82 105 130  58 130  83  84  60  76]
 [127 106 100  77 111 101  97 121  43  75]
 [122  73  17  71 115  72 142 105 170 141]
 [ 59  72 104 136 150 112  84  97  81  79]
 [113  74  45  67 140  92  96 193  95  94]]

10.63 of test examples classified correctly. k = 5
[[282 244  42  77  63  76  58  77  24  37]
 [170 241 207 120 110  68  20  71  45  83]
 [266 264 102  73  49  50  75 101  16  36]
 [ 91 177 302 100  64  60  25  90  22  79]
 [259 204  55  45  59  35  76  72  91  86]
 [191 168 151 108  35  68  43  60  30  38]
 [272 207 118  52  73  48  65  96   7  20]
 [240 147  48  52 113  40  73  54 164  97]
 [133 140 134 144 120  91  53  77  46  36]
 [264 129  44  31 104  83  76 179  53  46]]

Legend: left-right is predicted. Up to down is actual.

Why would it be so low? Given that we got near perfect score on two class classification. Well I believe it happens since we have only 100 examples per class.
Why then would the other thing be so damned high? Well it is radically easy to differentiate b/w 0 and 1. Probably the most distinct of all numbers. Here the contention is b/w 10.
But a probability of 10% simply means that the labels are being assigned randomly. Weird.




#### Q2 c - cross validation:
NOTE: Testing done on the trainset's splits as well. (800,200) splits. NOT tested on x_test
o/p (accuracy) of each split:
[ 0.135  0.125  0.125  0.14   0.125  0.105  0.135  0.135  0.125  0.115  0.11   0.14   0.145  0.135  0.125]
[ 0.11   0.11   0.075  0.11   0.115  0.115  0.115  0.09   0.11   0.115  0.065  0.07   0.08   0.075  0.105]
[ 0.095  0.105  0.09   0.085  0.075  0.095  0.095  0.105  0.105  0.125  0.125  0.115  0.12   0.12   0.11 ]
[ 0.105  0.11   0.07   0.08   0.085  0.08   0.1    0.095  0.085  0.08   0.09   0.095  0.08   0.095  0.09 ]
[ 0.125  0.11   0.105  0.105  0.065  0.085  0.08   0.09   0.1    0.12   0.11   0.12   0.135  0.135  0.145]

avg:
[ 0.114  0.112  0.093  0.104  0.093  0.096  0.105  0.103  0.105  0.111  0.1    0.108  0.112  0.112  0.115]

winner: k = 15

random since k = 7 was a winner sometime back. Also this is counter intuitive to the previous experiment. Very very weird.
Or maybe... I have cracked the code to generate pure random numbers.

**plot**: 5fcv_results


#### Q2 d - All classes, all examples
NOTE: It is ambigous whether I have to perform cross validation, or not. I am assuming that I do not. 
Using X_train, X_test in this case now.

<Experiment doesn't fit in RAM :/ 
	takes north of an hour to run>


# Logistic Regression

### Q1: Derivatives

_s(x)_ = 1 / (1 + e^(-x) )

u = wx + b
v = s( u )
l = (y - v)^2

dl/dv = -2 ( y - v )
dl/du = dl/dv * dv/du
      = dl/dv * s(u) * (1 - s(u))
**dl/dw = x dl/du**
**dl/db = dl/du**

### Q2: Implement

Done
`99.802605606 % of test examples classified correctly.`

### Q3: Plot
`99.9368337939 % of test examples classified correctly.`
t1000_sigmoid_results.png -> plot